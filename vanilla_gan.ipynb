{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla GAN on MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Sequential\n",
    "import torch.optim as optim\n",
    "from torch.autograd.variable import Variable\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torchvision import datasets,transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "import imageio\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loading MNIST data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,),(0.5,))\n",
    "])\n",
    "to_image = transforms.ToPILImage()\n",
    "trainset = datasets.MNIST(root=\"./mnist_data\",train=True,download=True,transform=transform)\n",
    "\n",
    "trainloader = DataLoader(trainset,batch_size=64,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Class Discriminator and Generator\n",
    "For discriminator we give an image(flattend) of size 784 and predict whether is it true/fake digit from MNIST. \n",
    "For Generator we give a radom noise of 100 dimensional data and predict a image(flattend) of size 784 to produce an digit inorder to fool discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.fc0 = Sequential(\n",
    "                        nn.Linear(784,1024),\n",
    "                        nn.LeakyReLU(0.2),\n",
    "                        nn.Dropout(0.3)\n",
    "                    )\n",
    "        self.fc1 = Sequential(\n",
    "                    nn.Linear(1024,512),\n",
    "                    nn.LeakyReLU(0.2),\n",
    "                    nn.Dropout(0.3)\n",
    "                    )\n",
    "        self.fc2 = Sequential(\n",
    "                    nn.Linear(512,128),\n",
    "                    nn.LeakyReLU(0.2),\n",
    "                    nn.Dropout(0.3)\n",
    "                    )\n",
    "        self.fc3 = Sequential(\n",
    "                    nn.Linear(128,1),\n",
    "                    nn.Sigmoid()\n",
    "                    )\n",
    "    def forward(self,x): # x is shape of 28X28 image tensor\n",
    "        x = x.view(-1,784)\n",
    "        x = self.fc0(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator,self).__init__()\n",
    "        self.fc0 = Sequential(\n",
    "                    nn.Linear(128,256),\n",
    "                    nn.LeakyReLU(0.2)\n",
    "                    )\n",
    "        self.fc1 = Sequential(\n",
    "                    nn.Linear(256,512),\n",
    "                    nn.LeakyReLU(0.2)\n",
    "                    )\n",
    "        self.fc2 = Sequential(\n",
    "                    nn.Linear(512,1024),\n",
    "                    nn.LeakyReLU(0.2)\n",
    "                    )\n",
    "        self.fc3 = Sequential(\n",
    "                    nn.Linear(1024,784)          \n",
    "                    )\n",
    "\n",
    "    def forward(self,x): # x is noise of 100 dimensions\n",
    "        x = self.fc0(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = x.view(-1,1,28,28)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "g_optim = optim.Adam(generator.parameters(),lr=1e-4,weight_decay=1e-6)\n",
    "d_optim = optim.Adam(discriminator.parameters(),lr=1e-4,weight_decay=1e-6)\n",
    "critetion = nn.BCELoss()\n",
    "g_losses=[]\n",
    "d_losses=[]\n",
    "images = []\n",
    "\n",
    "def noise(n,n_features=128):\n",
    "    return Variable(torch.rand(n,n_features)).to(device)\n",
    "\n",
    "def make_ones(size):\n",
    "    data = Variable(torch.ones(size,1))\n",
    "    return data.to(device)\n",
    "\n",
    "def make_zeros(size):\n",
    "    data = Variable(torch.zeros(size,1))\n",
    "    return data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator must return sum of loss of real and fake data\n",
    "def train_discriminator(optimizer,real_data,fake_data):\n",
    "    n=real_data.size(0)\n",
    "    optimizer.zero_grad()\n",
    "    predicted_real = discriminator(real_data)\n",
    "    real_loss = critetion(predicted_real,make_ones(n))\n",
    "    real_loss.backward()\n",
    "    \n",
    "    \n",
    "    predicted_fake = discriminator(fake_data)\n",
    "    fake_loss = critetion(predicted_fake,make_zeros(n))\n",
    "    fake_loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    return real_loss + fake_loss\n",
    "\n",
    "def train_generator(optimizer,fake_data):\n",
    "    n=fake_data.size(0)\n",
    "    optimizer.zero_grad()\n",
    "    predicted_fake = discriminator(fake_data)\n",
    "    fake_loss = critetion(predicted_fake,make_zeros(n))\n",
    "    fake_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return fake_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-61e2aa9aa523>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0md_error\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_optim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreal_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfake_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mfake_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mg_error\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_optim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfake_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_noise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-4700f159d51e>\u001b[0m in \u001b[0;36mtrain_generator\u001b[0;34m(optimizer, fake_data)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mpredicted_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mfake_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritetion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_fake\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmake_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mfake_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs=3\n",
    "test_noise = noise(64)\n",
    "generator.train()\n",
    "discriminator.train()\n",
    "for epoch in range(epochs):\n",
    "    g_error = 0.0\n",
    "    d_error = 0.0\n",
    "    for i,data in enumerate(trainloader):\n",
    "        inputs,target = data\n",
    "        real_data = inputs.to(device)\n",
    "        n = inputs.size(0)\n",
    "        fake_data = generator(noise(n)).detach()\n",
    "        d_error += train_discriminator(d_optim,real_data,fake_data)\n",
    "        fake_data = generator(noise(n))\n",
    "        g_error += train_generator(g_optim,fake_data)\n",
    "    \n",
    "    img = generator(test_noise).cpu().detach()\n",
    "    img = make_grid(img)\n",
    "    images.append(img)\n",
    "    print(i)\n",
    "    g_losses.append(g_error/i)\n",
    "    d_losses.append(d_error/i)\n",
    "    print(\"Epoch {}: g_loss is {:.6f} and d_loss is {:.6f} \".format(epoch,g_error/i,d_error/i))\n",
    "\n",
    "print(\"Finished Training\")\n",
    "torch.save(generator.state_dict(),\"mnist_generator.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = [np.array(to_image(i)) for i in images]\n",
    "imageio.mimsave('progress.gif', imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(g_losses,label=\"generator loss\")\n",
    "plt.plot(d_losses,label=\"discriminator loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
